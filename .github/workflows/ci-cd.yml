name: CICD Pipeline

on:
  push:
    branches: ["main"]
  pull_request:

env:
  REGISTRY: ghcr.io
  REF_REPO: ${{ github.repository }}
  DOCKER_BUILDKIT: 1
  FEAST_REPO_PATH: ./data-pipeline/churn_feature_store/churn_features/feature_repo
  TRAINING_REPO_PATH: ./model_pipeline
  SERVING_REPO_PATH: ./serving_pipeline

jobs:
  feast:
    runs-on: [self-hosted, linux]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Miniconda
        uses: conda-incubator/setup-miniconda@v3
        with:
          auto-update-conda: true
          python-version: 3.11
          auto-activate: true
          activate-environment: mlops_data

      - name: Restore packages
        shell: bash
        working-directory: ./data-pipeline
        run: |
          conda run -n mlops_data pip install -r requirements.txt
      - name: Restore files via DVC
        shell: bash
        working-directory: ./data-pipeline
        run: |
          conda run -n mlops_data dvc pull
      - name: Apply Feast Feature Definitions
        shell: bash
        working-directory: ${{ env.FEAST_REPO_PATH }}
        run: |
          conda run -n mlops_data feast apply

      - name: Materialize Feast Features
        shell: bash
        working-directory: ${{ env.FEAST_REPO_PATH }}
        run: |
          conda run -n mlops_data feast materialize-incremental $(date -u +"%Y-%m-%dT%H:%M:%S")

  training:
    runs-on: [self-hosted, linux]
    needs: feast
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Miniconda
        uses: conda-incubator/setup-miniconda@v3
        with:
          auto-update-conda: true
          python-version: 3.11
          auto-activate: true
          activate-environment: mlops_modeling
      
      - name: Setup S3 credentials
        run: |
          echo "AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}" >> $GITHUB_ENV
          echo "AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}" >> $GITHUB_ENV
          echo "MLFLOW_S3_ENDPOINT_URL=${{ vars.AWS_ENDPOINT_URL_S3 }}" >> $GITHUB_ENV

      - name: Restore packages
        shell: bash
        working-directory: ./model_pipeline
        run: |
          conda run -n mlops_modeling pip install -r requirements.txt
      - name: Restore files via DVC
        shell: bash
        working-directory: ./data-pipeline
        run: |
          conda run -n mlops_data dvc pull
      - name: Run Training
        shell: bash
        working-directory: ${{ env.TRAINING_REPO_PATH }}
        run: |
          conda run -n mlops_modeling bash ./src/run_sh/train.sh --config ./src/config/xgboost.yaml | tee train.log
          RUN_ID=$(grep -oE 'runs/[a-f0-9]+' train.log | head -n1 | cut -d/ -f2)
          echo "RUN_ID=$RUN_ID" >> $GITHUB_ENV
          echo "MODEL_URI=runs:${RUN_ID}/xgboost_churn" >> $GITHUB_ENV

      - name: Debug RUN_ID
        run: echo "The RUN_ID is ${{ env.RUN_ID }}"

      - name: Run Evaluation
        shell: bash
        working-directory: ${{ env.TRAINING_REPO_PATH }}
        run: |
          conda run -n mlops_modeling bash ./src/run_sh/eval.sh --config ./src/config/xgboost.yaml --run-id ${{ env.RUN_ID }} | tee eval.log

      - name: Run Registration
        shell: bash
        working-directory: ${{ env.TRAINING_REPO_PATH }}
        run: |
          conda run -n mlops_modeling bash ./src/run_sh/register.sh register --config ./src/config/xgboost.yaml --run-id ${{ env.RUN_ID }} --model-name xgboost-model  | tee register.log

      - name: Update Model Version Alias
        shell: bash
        working-directory: ${{ env.TRAINING_REPO_PATH }}
        run: |
          conda run -n mlops_modeling bash ./src/run_sh/set_model_alias.sh --config ./src/config/xgboost.yaml --model-name xgboost-model 

      - name: Promote to Champion
        shell: bash
        working-directory: ${{ env.TRAINING_REPO_PATH }}
        run: |
          conda run -n mlops_modeling bash ./src/run_sh/promote.sh --config ./src/config/xgboost.yaml --model-name xgboost-model

  serving:
    runs-on: [self-hosted, linux]
    needs: training

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Extract metadata (tags, labels) for Docker
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.REF_REPO }}
          tags: |
            # Tag for main branch
            type=raw,value=main,enable=${{ github.ref == 'refs/heads/main' }}
            # Tag with commit SHA
            type=sha,prefix={{branch}}-

      - name: Log in to the Container registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push Docker image
        id: push
        uses: docker/build-push-action@v6
        with:
          context: ${{ env.SERVING_REPO_PATH }}
          push: true
          platforms: linux/amd64
          tags: ${{ steps.meta.outputs.tags }}
          cache-from: type=registry,ref=${{ env.REGISTRY }}/${{ env.REF_REPO }}:buildcache
          cache-to: type=registry,ref=${{ env.REGISTRY }}/${{ env.REF_REPO }}:buildcache,mode=max

      - name: Start API Service
        shell: bash
        working-directory: ${{ env.SERVING_REPO_PATH }}
        run: |
          docker compose up -d --force-recreate
